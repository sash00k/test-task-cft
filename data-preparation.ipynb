{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\npaths = list()\nbase_path = '/kaggle/input/uplift-shift-23/x5-uplift-valid/'\nfor dirname, _, filenames in os.walk('/kaggle/working/'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        paths.append(path)\n        print(path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-06T15:56:44.112141Z","iopub.execute_input":"2023-01-06T15:56:44.112787Z","iopub.status.idle":"2023-01-06T15:56:44.150946Z","shell.execute_reply.started":"2023-01-06T15:56:44.112675Z","shell.execute_reply":"2023-01-06T15:56:44.149112Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"# это OHE, которая оставляется только лейблы с определенным уровнем представленности\ndef my_dummies(initial_column, threshold = None, sparse = False):\n    column = initial_column.copy()\n    if threshold is not None:\n        count = pd.value_counts(column)\n        filtred_column = count[count > threshold] # возможно, есть проблема с кодировкой np.NaN\n        mask = column.isin(filtred_column.index)\n        column[~mask] = 'DELETE_ME'\n        print(f'{column.name: >10}: {mask.sum(): >5} of {len(column): >5} ({mask.sum()/len(column):0.7f}) '\\\n              f'making {len(filtred_column)}/{len(initial_column.unique())-1} columns')\n    else:\n        print(f'{column.name: >10}: is fully encoded in {len(column.unique())} columns')\n        \n    return pd.get_dummies(column, prefix=column.name, prefix_sep='_', sparse=sparse).drop(f'{column.name}_DELETE_ME', axis=1, errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2023-01-06T15:56:44.153347Z","iopub.execute_input":"2023-01-06T15:56:44.154180Z","iopub.status.idle":"2023-01-06T15:56:44.164589Z","shell.execute_reply.started":"2023-01-06T15:56:44.154132Z","shell.execute_reply":"2023-01-06T15:56:44.162756Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# пригодится для приведения данных к численном виду\ndef safe_timestamp(timestamp) -> int:\n    try:\n        return timestamp.timestamp()\n    except ValueError:\n        return np.nan","metadata":{"execution":{"iopub.status.busy":"2023-01-06T15:56:44.166531Z","iopub.execute_input":"2023-01-06T15:56:44.167020Z","iopub.status.idle":"2023-01-06T15:56:44.181164Z","shell.execute_reply.started":"2023-01-06T15:56:44.166976Z","shell.execute_reply":"2023-01-06T15:56:44.179841Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%time\n# читаем данные о клиентах\nclients = pd.read_csv(f'{base_path}data/clients2.csv').drop(columns=['client_id.1'])\n\n# приводим даты к численному виду\nclients['first_issue_date'] = pd.to_datetime(clients['first_issue_date'], format='%Y-%m-%d %H:%M:%S').apply(safe_timestamp)\nclients['first_redeem_date'] = pd.to_datetime(clients['first_redeem_date'], format='%Y-%m-%d %H:%M:%S').apply(safe_timestamp)\n\n# заполняем пропущенные даты нулями\nclients['first_redeem_date'].fillna(0)\n\n# OHE пола клиентов\nclients = pd.concat([clients, pd.get_dummies(clients['gender'], prefix='gender', prefix_sep='_')], axis=1).drop(['gender', 'gender_U'], axis=1)\n\n# устаналиваем идентификатор клиента в качестве индекса\nclients.set_index('client_id', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T15:56:44.184123Z","iopub.execute_input":"2023-01-06T15:56:44.184655Z","iopub.status.idle":"2023-01-06T15:56:47.479847Z","shell.execute_reply.started":"2023-01-06T15:56:44.184607Z","shell.execute_reply":"2023-01-06T15:56:47.478492Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CPU times: user 2.92 s, sys: 183 ms, total: 3.1 s\nWall time: 3.28 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# читаем данные о товарах, делая OHE с учетом предствленности лейблов\nINF = 10 ** 10 # пороговое число, при указании которого у нас ничто не закодируется\nproducts = pd.read_csv(f'{base_path}data/products.csv')\nfor column_name, num in (('level_1', None),('level_2', 200),('level_3', 200),('level_4', 200),('segment_id', 300),('vendor_id', INF),('brand_id', 500)):\n    products = pd.concat([products, my_dummies(products[column_name], num)], axis=1).drop(column_name, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T15:56:47.481880Z","iopub.execute_input":"2023-01-06T15:56:47.482678Z","iopub.status.idle":"2023-01-06T15:56:47.949761Z","shell.execute_reply.started":"2023-01-06T15:56:47.482611Z","shell.execute_reply":"2023-01-06T15:56:47.948236Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   level_1: is fully encoded in 4 columns\n   level_2: 42069 of 43038 (0.9774850) making 26/42 columns\n   level_3: 34341 of 43038 (0.7979228) making 48/201 columns\n   level_4: 23889 of 43038 (0.5550676) making 47/790 columns\nsegment_id: 32543 of 43038 (0.7561457) making 40/116 columns\n vendor_id:     0 of 43038 (0.0000000) making 0/3193 columns\n  brand_id:  7415 of 43038 (0.1722896) making 2/4296 columns\nCPU times: user 342 ms, sys: 43.9 ms, total: 386 ms\nWall time: 458 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\nversion = 'v4'\nchunk = 10 ** 6\n\nfor iteration_num in range(16+7):  # сдвиг, с помощью которого мы по частями обрабатываем данные\n    data_type = 'train_purch' if iteration_num < 16 else 'test_purch' # 15`998`952 + 6`883`738\n    print(f'{iteration_num+1: >2} iteration, {data_type}.csv ...')\n    \n    # читаем чанк данных одного из файлов\n    purch_data = pd.read_csv(f'{base_path}{data_type}/{data_type}.csv', skiprows=(chunk)*(iteration_num%16)+1, nrows=chunk, \n                             names=['client_id', 'transaction_id', 'transaction_datetime', 'regular_points_received', \n                                    'express_points_received', 'regular_points_spent', 'express_points_spent', \n                                    'purchase_sum', 'store_id', 'product_id', 'product_quantity', 'trn_sum_from_iss',\n                                    'trn_sum_from_red'])\n    \n    # меняем purchase_sum на сумму с учетом product_quantity\n    purch_data['purchase_true_sum'] = purch_data.apply(lambda df: df['purchase_sum'] * df['product_quantity'], axis=1)\n\n    # учитываем что-то типа популярности магазина (ясно, что обрабатывая данные чанками получаем лишь часть статистики)\n    store_id_count = pd.value_counts(purch_data['store_id']).to_dict()\n    purch_data['store_count'] = purch_data['store_id'].apply(lambda x: store_id_count[x]) \n    purch_data.drop('store_id', axis=1, inplace=True)\n\n    # конвертация времени в чиселку\n    purch_data['transaction_datetime'] = pd.to_datetime(purch_data['transaction_datetime'], format='%Y-%m-%d %H:%M:%S').apply(safe_timestamp)\n\n    # кажется, тут можно просто заполнять нулями\n    purch_data.fillna(0, inplace=True)\n\n    # соединяем с данными о продуктах\n    purch_data = purch_data.merge(products, how='left', on='product_id').drop('product_id', axis=1)\n    \n    # заготавливаем словарик с функциями агрегации\n    agg_trans = {name: 'mean' for name in purch_data.columns if name != 'transaction_id'}\n    agg_trans['store_count'] = agg_trans['transaction_datetime'] = agg_trans['client_id'] = 'first'\n    agg_trans['purchase_sum'] = agg_trans['purchase_true_sum'] = agg_trans['product_quantity'] = ['sum', 'mean']\n\n    # применяем подготовленные аггрегационные функции\n    grouped_trans = purch_data.groupby('transaction_id')\n    transactions = pd.concat([grouped_trans.agg(agg_trans), grouped_trans['product_quantity'].size()], axis=1)\n\n    # переходим от tuple к str в качестве имен колонок\n    transactions.rename(columns={column: '_'.join(column) for column in transactions.columns if isinstance(column, tuple)}, inplace=True)\n\n    # усредняем данные по каждому клинету (кое-что суммируем)\n    grouped_clients = transactions.groupby('client_id_first')\n    clients_purches = pd.concat([grouped_clients.mean(), \n                                 grouped_clients['product_quantity'].size(), # кол-во походов в магазин\n                                 grouped_clients['product_quantity_sum'].sum(), # общее кол-во купленных товаров\n                                 grouped_clients['purchase_true_sum_sum'].sum(), # общее кол-во потраченных денег\n                                 grouped_clients['purchase_sum_sum'].sum(), # общее кол-во потраченных денег (не учитывает кол-во товара)\n                                 grouped_clients['transaction_datetime_first'].std(), # \"кучность\" походов в магазин\n                                 grouped_clients['transaction_datetime_first'].max() - \n                                 grouped_clients['transaction_datetime_first'].min() # период, в течение которого клиент ходил в магазин\n                                 ], axis=1)\n    \n    # переименуем индекс-колонку\n    clients_purches.index.rename('client_id', inplace=True)\n\n    # добавляем информацию о самих клиентах \n    result = clients.merge(clients_purches, how='right', on='client_id')\n    \n    # пишем один чанк данных\n    result.to_csv(f'/kaggle/working/purches_{version}.csv', mode='w' if iteration_num == 0 else 'a', header=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T15:56:47.952611Z","iopub.execute_input":"2023-01-06T15:56:47.952966Z","iopub.status.idle":"2023-01-06T16:12:23.599328Z","shell.execute_reply.started":"2023-01-06T15:56:47.952937Z","shell.execute_reply":"2023-01-06T16:12:23.598373Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":" 1 iteration, train_purch.csv ...\n 2 iteration, train_purch.csv ...\n 3 iteration, train_purch.csv ...\n 4 iteration, train_purch.csv ...\n 5 iteration, train_purch.csv ...\n 6 iteration, train_purch.csv ...\n10 iteration, train_purch.csv ...\n11 iteration, train_purch.csv ...\n12 iteration, train_purch.csv ...\n13 iteration, train_purch.csv ...\n14 iteration, train_purch.csv ...\n15 iteration, train_purch.csv ...\n16 iteration, train_purch.csv ...\n17 iteration, test_purch.csv ...\n18 iteration, test_purch.csv ...\n19 iteration, test_purch.csv ...\n20 iteration, test_purch.csv ...\n21 iteration, test_purch.csv ...\n22 iteration, test_purch.csv ...\n23 iteration, test_purch.csv ...\nCPU times: user 14min 30s, sys: 35.6 s, total: 15min 6s\nWall time: 15min 35s\n","output_type":"stream"}]},{"cell_type":"code","source":"# смотрим, что мы там сохранили\npurches = pd.read_csv(f'/kaggle/working/purches_{version}.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T16:12:23.600637Z","iopub.execute_input":"2023-01-06T16:12:23.600992Z","iopub.status.idle":"2023-01-06T16:12:29.227987Z","shell.execute_reply.started":"2023-01-06T16:12:23.600961Z","shell.execute_reply":"2023-01-06T16:12:29.226889Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# ну вот на стыках чанков у нас получилось разделение списков покупок покупателей (21 таких)\nclients_ids = purches.iloc[:, 0].tolist()\nprint(len(clients_ids), len(set(clients_ids)))\n\n# и это как раз согласуется с количеством уникальных client_id\nprint(pd.read_csv(f'{base_path}data/train.csv').shape[0] + pd.read_csv(f'{base_path}data/test.csv').shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-06T16:12:29.229843Z","iopub.execute_input":"2023-01-06T16:12:29.230755Z","iopub.status.idle":"2023-01-06T16:12:29.431646Z","shell.execute_reply.started":"2023-01-06T16:12:29.230711Z","shell.execute_reply":"2023-01-06T16:12:29.430464Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"200060 200039\n200039\n","output_type":"stream"}]},{"cell_type":"code","source":"# просто выкинем любую из дублирующих строк (21 испорченный сэмпл - не так страшно)\npurches.drop_duplicates(subset=[0], inplace=True)\n\n# надо еще в паре столбцов заполнить пропуски\npurches.fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T16:23:52.816415Z","iopub.execute_input":"2023-01-06T16:23:52.817723Z","iopub.status.idle":"2023-01-06T16:23:53.157998Z","shell.execute_reply.started":"2023-01-06T16:23:52.817676Z","shell.execute_reply":"2023-01-06T16:23:53.157073Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# запишем итоговый предобработанный датасет\npurches.to_csv(f'/kaggle/working/purches_{version}.csv', header=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T16:12:29.709000Z","iopub.execute_input":"2023-01-06T16:12:29.709304Z","iopub.status.idle":"2023-01-06T16:13:03.672183Z","shell.execute_reply.started":"2023-01-06T16:12:29.709277Z","shell.execute_reply":"2023-01-06T16:13:03.671090Z"},"trusted":true},"execution_count":10,"outputs":[]}]}